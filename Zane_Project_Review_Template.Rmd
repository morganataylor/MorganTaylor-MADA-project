---
title: Project Review Template 
date: "`r file.mtime(knitr::current_input())`"
#bibliography: ../media/references.bib
output: 
  html_document:
    toc_depth: 3
    number_sections: true
---

# Overview

Title of project: Predicting Federal Emergency Management Agency Funding for U.S. Disasters

Name of project author(s): Morgan Taylor

Name of project reviewer: Zane Billings

# Specific project content evaluation

## Background, Context and Motivation

### Feedback and Comments

* The paper includes a fairly comprehensive background of the disaster response process and how FEMA money is requested and allocated. The motivation for the project is clear.
* The only previous context for the research question is that "little has been done to draw conclusions from these data for predicting the financial impacts of disasters". I think it would benefit the paper to include a section on previous work for modeling disease costs, or mentioning any studies that have previously used this data in a similar way.
* The new information the project hopes to provide is explained well.
* Other than the lack of previous research context, I think the introduction is comprehensive. If there are truly no studies in this area, that should be noted as well.
* The section on machine learning models should be moved to the Methods section.

### Summary assessment

Some contextualization and motivation.

## Question description

### Feedback and Comments

* The question is explained well. What I understood as the research question is:

> Can the amount of allocated or requested FEMA funding be predicted for a given disaster?

> If this is not the intended research question, I advise revision of the introduction section.

* For me, the introduction would flow better if the paragraph beginning "It would greatly benefit the federal government to be able to predict..." was at the end of the introduction as this paragraph contains the statement of the research question.
* I think the research question is clear, but I think it could be made by more detailed by saying something to the effect of "We aim to predict allocated/predicted FEMA funds using...[list of predictors]." Not every predictor needs to be listed but I think this would be nice (overlaps somewhat with a point I make in the next section).

### Summary assessment
Question/hypotheses fully clear.


## Data description

### Feedback and Comments

* The data source is explicitly listed.
* The data are described well but I would prefer an explicit list of candidate predictors used for the models.
* I wasn't aware that codebooks were a requirement for the project, but codebooks are provided for two of the three raw data sheets, and links to the data sources are provided in the README.

### Summary assessment
Source and overall structure of data well explained.


## Data wrangling and exploratory analysis
How well is the data cleaned/processed and explored? Are all steps reasonable and well explained? Are alternatives discussed and considered? Are meaningful exploratory results shown (e.g. in the supplementary materials)?

### Feedback and Comments

WRITE YOUR FEEDBACK HERE

### Summary assessment
* major weaknesses in wrangling and exploratory component
* some weaknesses in wrangling and exploratory component
* essentially no weaknesses in wrangling and exploratory component



## Appropriateness of Analysis
Were the analysis methods appropriate for the data? Was the analysis done properly? Were different components of the analysis (e.g. performance measure, variable selection, data pre-processing, model evaluation) done in the best way possible and explained well?

### Feedback and Comments

If time permits, I would recommend consideration of a few other modeling methods. I think regression with the elastic net should be included--this tends to be a logical "next step" from multiple linear regression, and is typically quite fast to fit. The model is also easy to interpret, and usually I think if an elastic net model fits "about as well" as a tree-based model, the elastic net model should be preferred. 

KNN or a gradient-boosted tree might also be a nice addition. In some cases, gradient-boosted trees can generate more accurate predictions than random forests, but can also be easier to overfit and typically take a long time to train. On the other hand, KNN is very easy to train (one parameter, unless you count the distance function) and the performance depends entirely on the amount of local structure in the data, so it might or might not be useful.

That being said,

### Summary assessment
* wrong/inadequate analysis
* defensible but not optimal analysis 
* strong and reasonable analysis

## Presentation
How well are results presented? Are tables and figures easy to read and understand? Are the main figures/tables publication level quality? 

### Feedback and Comments

WRITE YOUR FEEDBACK HERE

### Summary assessment
* results are poorly presented, hard to understand, poor quality
* results are presented ok, with room for improvement
* results are very well presented


## Discussion/Conclusions
Are the study findings properly discussed? Are strengths and limitations acknowledged? Are findings interpreted properly?

### Feedback and Comments

WRITE YOUR FEEDBACK HERE

### Summary assessment
* major parts of discussion missing or wrong 
* minor parts wrong, missing or unclear
* strong, complete and clear discussion


## Further comments

_Add any other comments regarding the different aspects of the project here. Write anything you think can help your classmate improve their project._

# Overall project content evaluation
Evaluate overall features of the project  by filling in the sections below.


## Structure
Is the project well structured? Are files in well labeled folders? Do files have reasonable names? Are all "junk" files not needed for analysis/reproduction removed? By just looking at files and folders, can you get an idea of how things fit together?

### Feedback and Comments

WRITE YOUR FEEDBACK HERE

### Summary assessment
* poor and confusing structure
* mostly clear, but some confusing parts (e.g. useless files, things in the wrong folders)
* well structured


## Documentation 
How well is the project documented? Are you able to understand each step of the whole analysis, each decision that was made, and each line of code? Is enough information provided as comments in code or as part of Rmd files? 

### Feedback and Comments

WRITE YOUR FEEDBACK HERE

### Summary assessment
* poorly documented
* decently documented with some gaps
* fully and well documented



## Reproducibility
Are all results fully reproducible? Is documentation provided which clearly explains how to reproduce things, and does it work without the need for any manual intervention? Are you able to re-run the whole analysis without having to do manual interventions/edits?

### Feedback and Comments

WRITE YOUR FEEDBACK HERE


### Summary assessment
* major parts not reproducible 
* small parts not reproducible or required manual intervention 
* fully reproducible without issues


## Thoroughness
How thorough was the overall study? Were alternatives (e.g. different ways of processing the data or different models) considered? Were alternatives discussed? Were the questions/hypotheses fully and thoroughly addressed?

### Feedback and Comments

WRITE YOUR FEEDBACK HERE

### Summary assessment
* weak level of thoroughness
* decent level of thoroughness
* strong level of thorougness


## Further comments

_Add any other comments regarding the overall project here. Write anything you think can help your classmate improve their project._

* Best not to index temporary Office files with Git, e.g. `~$nuscript_Disasters.docx`. These are indexed for Git tracking when you commit all staged changes to Git while you have an unsaved Office document open. Assuming that you don't use `$` in file names, you can add `*$*` to the `.gitignore` (or to be safer, `*$*.docx`).
* You can format an abstract in your document using YAML by adding the lines
    `abstract: |`
    `text of abstract goes down here`
* 



