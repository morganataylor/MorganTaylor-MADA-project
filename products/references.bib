
@misc{femaGuideDisasterDeclaration2021,
	title = {A {Guide} {To} {The} {Disaster} {Declaration} {Process} and {Federal} {Disaster} {Assistance}},
	url = {https://www.fema.gov/pdf/rrr/dec_proc.pdf},
	abstract = {Local and State governments share the responsibility for protecting their citizens from disasters, and for helping
them to recover when a disaster strikes. In some cases, a disaster is beyond the capabilities of the State and local
government to respond. In 1988, the Robert T. Stafford Disaster Relief and Emergency Assistance Act, 42 U.S.C. §§ 5121-5206, was enacted to support State and local governments and their citizens when disasters overwhelm them. This law, as amended, establishes a process for requesting and obtaining a Presidential disaster declaration, defines the type and
scope of assistance available from the Federal government, and sets the conditions for obtaining that assistance. The Federal Emergency Management Agency (FEMA), now part of the Emergency Preparedness and Response
Directorate of the Department of Homeland Security, is tasked with coordinating the response.
This paper explains the declaration process and provides an overview of the assistance available.},
	publisher = {Federal Emergency Management Agency (FEMA)},
	author = {FEMA},
	month = sep,
	year = {2021},
	file = {dec_proc.pdf:C\:\\Users\\mat71418\\Zotero\\storage\\JFR947DR\\dec_proc.pdf:application/pdf},
}

@article{brassingtonMeanAbsoluteError2017,
	title = {Mean absolute error and root mean square error: which is the better metric for assessing model performance?},
	shorttitle = {Mean absolute error and root mean square error},
	url = {https://ui.adsabs.harvard.edu/abs/2017EGUGA..19.3574B},
	abstract = {The mean absolute error (MAE) and root mean square error (RMSE) are two metrics that are often used interchangeably as measures of ocean forecast accuracy. Recent literature has debated which of these should be preferred though their conclusions have largely been based on empirical arguments. We note that in general, RM SE2 = M AE2 + V ARk [{\textbar}ɛ{\textbar}] PIC PIC such that RMSE includes both the MAE as well as additional information related to the variance (biased estimator) of the errors ɛ with sample size k. The greater sensitivity of RMSE to a small number of outliers is directly attributable to the variance of absolute error. Further statistical properties for both metrics are derived and compared based on the assumption that the errors are Gaussian. For an unbiased (or bias corrected) model both MAE and RMSE are shown to estimate the total error standard deviation to within a constant coefficient such that ° -- M AE ≈ 2/πRM SE PIC . Both metrics have comparable behaviour in response to model bias and asymptote to the model bias as the bias increases. MAE is shown to be an unbiased estimator while RMSE is a biased estimator. MAE also has a lower sample variance compared with RMSE indicating MAE is the most robust choice. For real-time applications where there is a likelihood of "bad" observations we recommend ° - ° ---° - π- -1- π- π- TESD = 2 M AE ± √k- 2 - 1 2M AE PIC as an unbiased estimator of the total error standard deviation with error estimates (one standard deviation) based on the sample variance and defined as a scaling of the MAE itself. A sample size (k) on the order of 90 and 9000 provides an error scaling of 10\% and 1\% respectively. Nonetheless if the model performance is being analysed using a large sample of delayed-mode quality controlled observations then RMSE might be preferred where the second moment sensitivity to large model errors is important. Alternatively for model intercomparisons the information might compactly represented by a graph with axes of MAE PIC and °V-ARk-[{\textbar}ɛ{\textbar}] PIC where radials from the origin represent RMSE PIC .},
	urldate = {2021-11-29},
	author = {Brassington, Gary},
	month = apr,
	year = {2017},
	note = {Conference Name: EGU General Assembly Conference Abstracts
ADS Bibcode: 2017EGUGA..19.3574B},
	pages = {3574},
	file = {Full Text PDF:C\:\\Users\\mat71418\\Zotero\\storage\\A9QDED3Q\\Brassington - 2017 - Mean absolute error and root mean square error wh.pdf:application/pdf},
}

@article{dietterichExperimentalComparisonThree,
	title = {An {Experimental} {Comparison} of {Three} {Methods} for {Constructing} {Ensembles} of {Decision} {Trees}: {Bagging}, {Boosting}, and {Randomization}},
	abstract = {Bagging and boosting are methods that generate a diverse ensemble of classiﬁers by manipulating the training data given to a “base” learning algorithm. Breiman has pointed out that they rely for their eﬀectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the eﬀectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classiﬁcation noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classiﬁcation noise, bagging is much better than boosting, and sometimes better than randomization.},
	language = {en},
	author = {Dietterich, Thomas G},
	pages = {22},
	file = {Dietterich - An Experimental Comparison of Three Methods for Co.pdf:C\:\\Users\\mat71418\\Zotero\\storage\\PFRM4X5Z\\Dietterich - An Experimental Comparison of Three Methods for Co.pdf:application/pdf},
}

@article{breimanRandomForests2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2021-11-29},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	pages = {5--32},
	file = {Springer Full Text PDF:C\:\\Users\\mat71418\\Zotero\\storage\\3VLMPYF5\\Breiman - 2001 - Random Forests.pdf:application/pdf},
}

@book{boehmkeHandsOnMachineLearning2019,
	title = {Hands-{On} {Machine} {Learning} with {R}},
	isbn = {978-1-138-49568-5},
	url = {https://www.routledge.com/Hands-On-Machine-Learning-with-R/Boehmke-Greenwell/p/book/9781138495685},
	abstract = {Hands-on Machine Learning with R provides a practical and applied approach to learning and developing intuition into today’s most popular machine learning methods. This book serves as a practitioner’s guide to the machine learning process and is meant to help the reader learn to apply the machine learning stack within R, which includes using various R packages such as glmnet, h2o, ranger, xgboost, keras, and others to effectively model and gain insight from their data. The book favor},
	language = {en},
	urldate = {2021-11-29},
	publisher = {Routledge},
	author = {Boehmke, Brad and Greenwell, Brandon M.},
	month = nov,
	year = {2019},
	file = {Snapshot:C\:\\Users\\mat71418\\Zotero\\storage\\4ECYJP9M\\9781138495685.html:text/html},
}

@book{jamesIntroductionStatisticalLearning2021,
	edition = {2},
	title = {An {Introduction} to {Statistical} {Learning} with {Applications} in {R}},
	isbn = {978-1-07-161417-4},
	abstract = {As the scale and scope of data collection continue to increase across virtually all fields, statistical learning has become a critical toolkit for anyone who wishes to understand data. An Introduction to Statistical Learning provides a broad and less technical treatment of key topics in statistical learning. Each chapter includes an R lab. This book is appropriate for anyone who wishes to use contemporary tools for data analysis.},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	month = jul,
	year = {2021},
}

@book{mccarthyFEMADisasterDeclaration2009,
	title = {{FEMA}'s {Disaster} {Declaration} {Process}: {A} {Primer}},
	isbn = {978-1-4379-3084-9},
	abstract = {Contents: (1) Background; (2) Congress and the Declaration Process: Impetus, and Skepticism for Reform; (3) Pres¿l. and Gubernatorial Discretion; (4) Preliminary Damage Assessments; (5) Factors Considered for Public Assistance in Major Disaster Declarations (MDD): Estimated Cost of the Assist.; Localized Impacts; Insur. Coverage; Hazard Mitigation; Recent Multiple Disasters; Other Fed. Programs; (6) Factors Considered for Individual Assist. in MDD: Concentration of Damages; Trauma; Special Populations; Voluntary Agency Assist.; Insur. Coverage; Avg. Amount of Individual Assist. by State; Congress. Consid. for the Declaration Process; Composition of Preliminary Damage Assessment Teams; and Revising Individual Assist. Averages.},
	publisher = {DIANE Publishing Company},
	author = {McCarthy, Francis X.},
	month = jan,
	year = {2009},
}

@article{mossStaffordActPriorities2009,
	title = {The {Stafford} {Act} and {Priorities} for {Reform}},
	volume = {6},
	abstract = {During the past fifty years, federal disaster policy in the United States has been shaped by
an ongoing conflict between proponents who favor federal intervention following a disaster and
those who believe disaster response should be the responsibility of state and local governments
and charity. This article explores the existing federal disaster policy landscape within the United
States with a focus on the Stafford Act, the cultural and political forces that produced it, and how
the current system is ill equipped to aid in the response and recovery from major catastrophes.
The Stafford Act defines how federal disasters are declared, determines the types of assistance to
be provided by the federal government, and establishes cost sharing arrangements among federal,
state, and local governments. The Federal Emergency Management Agency (FEMA) carries out
the provisions of the Stafford Act and distributes much of the assistance provided by the Act. With
the establishment of the U.S. Department of Homeland Security, the threat of domestic terrorism,
and large-scale natural disasters like Hurricane Katrina, the limits of the Stafford Act and FEMA
have been shown. We look at several areas where the shortcomings of the Stafford Act have
emerged and propose directions for reform.},
	number = {1},
	journal = {Journal of Homeland Security and Emergency Management},
	author = {Moss, Mitchell and Schellhamer, Charles and Berman, David A.},
	year = {2009},
	pages = {1--21},
}

@misc{rcoreteamLanguageEnvironmentStatistical2021,
	address = {Vienna, Austria},
	title = {R: {A} language and environment for statistical computing},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {R Core Team},
	year = {2021},
}

@misc{OpenFEMADataSets,
	title = {{OpenFEMA} {Data} {Sets} {\textbar} {FEMA}.gov},
	url = {https://www.fema.gov/about/openfema/data-sets},
	abstract = {Disaster Info Emergency Management Individual Assistance Public Assistance Hazard Mitigation NFIP Misc},
	language = {en},
	urldate = {2021-11-29},
	file = {Snapshot:C\:\\Users\\mat71418\\Zotero\\storage\\J5IZX8WG\\data-sets.html:text/html},
}

@misc{USDAERSCountylevel,
	title = {{USDA} {ERS} - {County}-level {Data} {Sets}},
	url = {https://www.ers.usda.gov/data-products/county-level-data-sets/},
	urldate = {2021-11-29},
	file = {USDA ERS - County-level Data Sets:C\:\\Users\\mat71418\\Zotero\\storage\\2SQA34L2\\county-level-data-sets.html:text/html},
}

@article{araujoperezPredictiveModelsDisaster2019,
	title = {Predictive models on disaster declarations in the {US}},
	copyright = {Attribution-NonCommercial-NoDerivs 3.0 United States},
	url = {https://repositorio.comillas.edu/xmlui/handle/11531/36607},
	abstract = {Todos los años, frente a las catástrofes naturales que tienen lugar en los Estados Unidos, el gobierno federal es responsable de ayudar a los ciudadanos tanto de forma económica como física para recuperarse de los daños causados. El hecho de que el evento sea declarado con la “Declaración Presidencial de Catástrofe Natural” o no, tiene un gran impacto económico tanto en el gobierno del estado como en sus ciudadanos.Razón por la que es muy importante entender bien cuáles son las condiciones o características que determinan la obtención de dicha declaración.
El objetivo de este estudio es construir un modelo científico para predecir la probabilidad de que una catástrofe natural dada conlleve una Declaración Presidencial de Catástrofe Natural en el estado de Maryland, utilizando los datos de los 15 últimos años (2003-2017).
El modelo de predicción va a ser construido empleando un modelo de regresión logístico, puesto que la variable dependiente es una decisión binaria (si la catástrofe es declarada o no). 
Las variables independientes incluidas en el análisis son las siguientes: Ingresos, Población, Lesiones Directas, Lesiones Indirectas, Fallecimientos Directos, Fallecimientos Indirectos, Tipo de Evento, Daño a los Cultivos y Daño a la Propiedad como variables independientes; y Declaración como variable dependiente binaria.
Los pasos a proceder son Análisis de variables, Construcción del Modelo, Bondad de Ajuste y Conclusiones. Todos estudiados con sus respectivos tests.
De entre los métodos de construcción del modelo backward elimination ha sido elegido.},
	language = {en-GB},
	urldate = {2021-12-13},
	author = {Araujo Pérez, María},
	year = {2019},
	note = {Accepted: 2019-05-09T10:13:14Z},
	file = {Snapshot:C\:\\Users\\mat71418\\Zotero\\storage\\P2VWFUQE\\36607.html:text/html;Full Text PDF:C\:\\Users\\mat71418\\Zotero\\storage\\KL8YU97Z\\Araujo Pérez - 2019 - Predictive models on disaster declarations in the .pdf:application/pdf},
}

@book{hastieElementsStatisticalLearning2016,
	edition = {2nd},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-84857-0},
	abstract = {This book describes the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of colour graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.

This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorisation, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates.},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2016},
}

@article{zouRegularizationVariableSelection2005,
	title = {Regularization and variable selection via the elastic net},
	volume = {67},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	abstract = {Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
	language = {en},
	number = {2},
	urldate = {2021-12-13},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	year = {2005},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00503.x},
	keywords = {Grouping effect, LARS algorithm, Lasso, p≫n problem, Penalization, Variable selection},
	pages = {301--320},
	file = {Full Text PDF:C\:\\Users\\mat71418\\Zotero\\storage\\LHDQD265\\Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf:application/pdf;Snapshot:C\:\\Users\\mat71418\\Zotero\\storage\\RLRILVPB\\j.1467-9868.2005.00503.html:text/html},
}

@article{rothGeneralizedLASSO2004,
	title = {The generalized {LASSO}},
	volume = {15},
	issn = {1941-0093},
	doi = {10.1109/TNN.2003.809398},
	abstract = {In the last few years, the support vector machine (SVM) method has motivated new interest in kernel regression techniques. Although the SVM has been shown to exhibit excellent generalization properties in many experiments, it suffers from several drawbacks, both of a theoretical and a technical nature: the absence of probabilistic outputs, the restriction to Mercer kernels, and the steep growth of the number of support vectors with increasing size of the training set. In this paper, we present a different class of kernel regressors that effectively overcome the above problems. We call this approach generalized LASSO regression. It has a clear probabilistic interpretation, can handle learning sets that are corrupted by outliers, produces extremely sparse solutions, and is capable of dealing with large-scale problems. For regression functionals which can be modeled as iteratively reweighted least-squares (IRLS) problems, we present a highly efficient algorithm with guaranteed global convergence. This defies a unique framework for sparse regression models in the very rich class of IRLS models, including various types of robust regression models and logistic regression. Performance studies for many standard benchmark datasets effectively demonstrate the advantages of this model over related approaches.},
	number = {1},
	journal = {IEEE Transactions on Neural Networks},
	author = {Roth, V.},
	month = jan,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Machine learning, Additive noise, Iterative algorithms, Kernel, Large-scale systems, Logistics, Regression analysis, Robustness, Support vector machines},
	pages = {16--28},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\mat71418\\Zotero\\storage\\ERMEYVXZ\\Roth - 2004 - The generalized LASSO.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\mat71418\\Zotero\\storage\\LKDQZTXG\\1263575.html:text/html},
}

@misc{googledeveloperStatesCsvDataset,
	title = {states.csv {\textbar} {Dataset} {Publishing} {Language}},
	url = {https://developers.google.com/public-data/docs/canonical/states_csv},
	language = {en},
	urldate = {2021-12-13},
	journal = {Google Developers},
	author = {Google Developer},
	file = {Snapshot:C\:\\Users\\mat71418\\Zotero\\storage\\IH97QVDB\\states_csv.html:text/html},
}

@Manual{here,
    title = {here: A Simpler Way to Find Your Files},
    author = {Kirill Müller},
    year = {2020},
    note = {R package version 1.0.1},
    url = {https://CRAN.R-project.org/package=here},
  }

@Article{tidyverse,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686},
  }
  
@Manual{skimr,
    title = {skimr: Compact and Flexible Summaries of Data},
    author = {Elin Waring and Michael Quinn and Amelia McNamara and Eduardo {Arino de la Rubia} and Hao Zhu and Shannon Ellis},
    year = {2021},
    note = {R package version 2.1.3},
    url = {https://CRAN.R-project.org/package=skimr},
  }

@Manual{spatialsample,
    title = {spatialsample: Spatial Resampling Infrastructure},
    author = {Julia Silge},
    year = {2021},
    note = {R package version 0.1.0},
    url = {https://CRAN.R-project.org/package=spatialsample},
  }
  
@Manual{broommixed,
    title = {broom.mixed: Tidying Methods for Mixed Models},
    author = {Ben Bolker and David Robinson},
    year = {2021},
    note = {R package version 0.2.7},
    url = {https://CRAN.R-project.org/package=broom.mixed},
  }
  
@Manual{rpartplot,
    title = {rpart.plot: Plot 'rpart' Models: An Enhanced Version of 'plot.rpart'},
    author = {Stephen Milborrow},
    year = {2021},
    note = {R package version 3.1.0},
    url = {https://CRAN.R-project.org/package=rpart.plot},
  }
  
@Article{vip,
    title = {Variable Importance Plots—An Introduction to the vip Package},
    author = {Brandon M. Greenwell and Bradley C. Boehmke},
    journal = {The R Journal},
    year = {2020},
    volume = {12},
    number = {1},
    pages = {343--366},
    url = {https://doi.org/10.32614/RJ-2020-013},
  }
  
@Manual{doParallel,
    title = {doParallel: Foreach Parallel Adaptor for the 'parallel' Package},
    author = {Microsoft Corporation and Steve Weston},
    year = {2020},
    note = {R package version 1.0.16},
    url = {https://CRAN.R-project.org/package=doParallel},
  }
  
@Article{ranger,
    title = {{ranger}: A Fast Implementation of Random Forests for High Dimensional Data in {C++} and {R}},
    author = {Marvin N. Wright and Andreas Ziegler},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {77},
    number = {1},
    pages = {1--17},
    doi = {10.18637/jss.v077.i01},
  }
  
@Manual{viridis,
    title = {{viridis} - Colorblind-Friendly Color Maps for R},
    author = {{Garnier} and {Simon} and {Ross} and {Noam} and {Rudis} and {Robert} and {Camargo} and Antônio Pedro and {Sciaini} and {Marco} and {Scherer} and {Cédric}},
    year = {2021},
    note = {R package version 0.6.2},
    url = {https://sjmgarnier.github.io/viridis/},
    doi = {10.5281/zenodo.4679424},
    doi = {10.5281/zenodo.4679424},
  }
  
@Manual{maps,
    title = {maps: Draw Geographical Maps},
    author = {Original S code by Richard A. Becker and Allan R. Wilks. R version by Ray Brownrigg. Enhancements by Thomas P Minka and Alex Deckmyn.},
    year = {2021},
    note = {R package version 3.4.0},
    url = {https://CRAN.R-project.org/package=maps},
  }
  
@Manual{table1,
    title = {table1: Tables of Descriptive Statistics in HTML},
    author = {Benjamin Rich},
    year = {2021},
    note = {R package version 1.4.2},
    url = {https://CRAN.R-project.org/package=table1},
  }
  
@Manual{cowplot,
    title = {cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'},
    author = {Claus O. Wilke},
    year = {2020},
    note = {R package version 1.1.1},
    url = {https://CRAN.R-project.org/package=cowplot},
  }
  
@Manual{scales,
    title = {scales: Scale Functions for Visualization},
    author = {Hadley Wickham and Dana Seidel},
    year = {2020},
    note = {R package version 1.1.1},
    url = {https://CRAN.R-project.org/package=scales},
  }
  
@Article{gtsummary,
    author = {Daniel D. Sjoberg and Karissa Whiting and Michael Curry and Jessica A. Lavery and Joseph Larmarange},
    title = {Reproducible Summary Tables with the gtsummary Package},
    journal = {{The R Journal}},
    year = {2021},
    url = {https://doi.org/10.32614/RJ-2021-053},
    doi = {10.32614/RJ-2021-053},
    volume = {13},
    issue = {1},
    pages = {570-580},
  }
  
@Book{car,
    title = {An {R} Companion to Applied Regression},
    edition = {Third},
    author = {John Fox and Sanford Weisberg},
    year = {2019},
    publisher = {Sage},
    address = {Thousand Oaks {CA}},
    url = {https://socialsciences.mcmaster.ca/jfox/Books/Companion/},
  }
  
@Book{ggplot2,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    isbn = {978-3-319-24277-4},
    url = {https://ggplot2.tidyverse.org},
  }
  
@Manual{summarytools,
    title = {summarytools: Tools to Quickly and Neatly Summarize Data},
    author = {Dominic Comtois},
    year = {2021},
    note = {R package version 1.0.0},
    url = {https://CRAN.R-project.org/package=summarytools},
  }
  
@Manual{tidymodels,
    title = {Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles.},
    author = {Max Kuhn and Hadley Wickham},
    url = {https://www.tidymodels.org},
    year = {2020},
  }